{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ma2kepJkhvm"
      },
      "outputs": [],
      "source": [
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "from google.colab import drive\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "\n",
        "\n",
        "drive.mount('/drive')\n",
        "nlp = spacy.load(\"ro_core_news_lg\")\n",
        "df = pd.read_json(\"hf://datasets/mihalca/Fakerom_updated_original/combined_data.json\")\n",
        "df['tag'] = df['tag'].replace({\n",
        "    \"misinformation\": \"fake_news\",\n",
        "    \"propaganda\": \"fake_news\"\n",
        "})\n",
        "df = df[df[\"tag\"] != \"satire\"]\n",
        "df_filtered = df[df['content'].str.count(r'\\b\\w+\\b') <= 550]\n",
        "\n",
        "\n",
        "model_name_bart = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer_bart = MBart50TokenizerFast.from_pretrained(model_name_bart)\n",
        "model_bart = MBartForConditionalGeneration.from_pretrained(model_name_bart)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_bart = model_bart.to(device)\n",
        "print(f\"Model loaded on: {device}\")\n",
        "\n",
        "def translate(text, src_lang, tgt_lang):\n",
        "\n",
        "    tokenizer_bart.src_lang = src_lang\n",
        "\n",
        "    encoded = tokenizer_bart(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_tokens = model_bart.generate(\n",
        "        **encoded,\n",
        "        forced_bos_token_id=tokenizer_bart.lang_code_to_id[tgt_lang]\n",
        "    )\n",
        "    return tokenizer_bart.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "back_translated_data = []\n",
        "cnt = 0\n",
        "for index, row in df_filtered.iterrows():\n",
        "    text = row['content']\n",
        "    tag = row['tag']\n",
        "    translated_text = translate(text, \"ro_RO\", \"en_XX\")\n",
        "    back_translated_text = translate(translated_text, \"en_XX\", \"ro_RO\")\n",
        "    cnt += 1\n",
        "    print(cnt)\n",
        "    back_translated_data.append({\n",
        "        'content': back_translated_text,\n",
        "        'tag': tag\n",
        "    })\n",
        "\n",
        "df_back_translated = pd.DataFrame(back_translated_data)\n",
        "\n",
        "df_augmented = pd.concat([df_filtered, df_back_translated], ignore_index=True)\n",
        "\n",
        "df_augmented.to_csv('/drive/My Drive/augmented_dataset_from_english.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "import spacy\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "drive.mount('/drive')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "nlp = spacy.load(\"ro_core_news_lg\")\n",
        "\n",
        "df = pd.read_json(\"hf://datasets/mihalca/Fakerom_updated_original/combined_data.json\")\n",
        "df['tag'] = df['tag'].replace({\n",
        "    \"misinformation\": \"fake_news\",\n",
        "    \"propaganda\": \"fake_news\"\n",
        "})\n",
        "df = df[df[\"tag\"] != \"satire\"]\n",
        "df = df[df['content'].str.count(r'\\b\\w+\\b') <= 550]\n",
        "\n",
        "def load_model(model_name):\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
        "    return tokenizer, model\n",
        "\n",
        "ro_fr_tok, ro_fr_model = load_model(\"Helsinki-NLP/opus-mt-ro-fr\")\n",
        "fr_ro_tok, fr_ro_model = load_model(\"Helsinki-NLP/opus-mt-fr-ro\")\n",
        "\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "def translate(texts, tokenizer, model):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        translated = model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,\n",
        "            num_beams=4,\n",
        "            early_stopping=False\n",
        "        )\n",
        "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "def back_translate(text):\n",
        "    sentences = split_into_sentences_spacy(text)\n",
        "    result = []\n",
        "    for sent in sentences:\n",
        "        try:\n",
        "            fr = translate([sent], ro_fr_tok, ro_fr_model)[0]\n",
        "            ro = translate([fr], fr_ro_tok, fr_ro_model)[0]\n",
        "            result.append(ro)\n",
        "        except:\n",
        "            print(\"fallback\")\n",
        "            result.append(sent)\n",
        "    return \" \".join(result)\n",
        "\n",
        "augmented_rows = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    original_text = row['content']\n",
        "    label = row['tag']\n",
        "    augmented_text = back_translate(original_text)\n",
        "    augmented_rows.append({\n",
        "        \"tag\": label,\n",
        "        \"content\": augmented_text\n",
        "    })\n",
        "\n",
        "df_augmented = pd.DataFrame(augmented_rows)\n",
        "\n",
        "df_augmented.to_csv('/drive/My Drive/augmented_dataset_from_french.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Vo38DPAMk5Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "from google.colab import drive\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "\n",
        "#PREPROCESSING\n",
        "drive.mount('/drive')\n",
        "nlp = spacy.load(\"ro_core_news_lg\")\n",
        "df_augmented = pd.read_csv('/drive/My Drive/augmented_dataset_from_french.csv')\n",
        "\n",
        "url_pattern = re.compile(r'https?://\\S+')\n",
        "def remove_urls(text):\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "\n",
        "df_augmented['content'] = df_augmented['content'].apply(remove_urls)\n",
        "\n",
        "df_augmented = df_augmented.replace(to_replace=r'\\d', value='', regex=True)\n",
        "\n",
        "def lemmatize_and_filter(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space])\n",
        "\n",
        "df_augmented['content'] = df_augmented['content'].apply(lemmatize_and_filter)\n",
        "\n",
        "\n",
        "df_augmented = df_augmented.map(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "df_augmented.to_csv('/drive/My Drive/augmented_dataset_final_preprocessed.csv', index=False)\n"
      ],
      "metadata": {
        "id": "EVSlcIZ3lcdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}