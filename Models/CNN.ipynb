{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9GxLT5_F3zf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#EXPERIMENT ONE\n",
        "drive.mount('/drive')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
        "\n",
        "max_length = 512\n",
        "def encode(text):\n",
        "    return tokenizer.encode(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "df = pd.read_csv(\"/drive/My Drive/dataset_experiment_one.csv\")\n",
        "\n",
        "\n",
        "encoded_sequences = df['content'].apply(encode)\n",
        "padded_sequences = np.array(encoded_sequences.tolist())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(df['tag'])\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(padded_sequences, labels, test_size=0.1, stratify=labels, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.11, stratify=y_temp, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.build(input_shape=(None, max_length))\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(f\"Test Accuracy: {accuracy:.5f}\")\n",
        "print(f\"Test Loss: {loss:.5f}\")\n",
        "\n",
        "model.save(\"/drive/My Drive/cnn_real_testing_train_and_test.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#EXPERIMENt TWO\n",
        "drive.mount('/drive')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
        "\n",
        "max_length = 512\n",
        "def encode(text):\n",
        "    return tokenizer.encode(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(\"/drive/My Drive/dataset_experiment_two_training.csv\")\n",
        "test_df = pd.read_csv(\"/drive/My Drive/dataset_experiment_two_testing.csv\")\n",
        "\n",
        "\n",
        "encoded_sequences_train = train_df['content'].apply(encode)\n",
        "padded_sequences_train = np.array(encoded_sequences_train.tolist())\n",
        "encoded_sequences_test = test_df['content'].apply(encode)\n",
        "padded_sequences_test = np.array(encoded_sequences_test.tolist())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(train_df['tag'])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences_train, labels, test_size=0.1, stratify=labels, random_state=42)\n",
        "X_test = padded_sequences_test\n",
        "y_test = label_encoder.transform(test_df['tag'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.build(input_shape=(None, max_length))\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(f\"Test Accuracy: {accuracy:.5f}\")\n",
        "print(f\"Test Loss: {loss:.5f}\")\n",
        "\n",
        "model.save(\"/drive/My Drive/cnn_experiment_two.keras\")"
      ],
      "metadata": {
        "id": "azg5HeI1GBr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#EXPERIMENT THREE\n",
        "drive.mount('/drive')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
        "\n",
        "max_length = 512\n",
        "def encode(text):\n",
        "    return tokenizer.encode(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "df = pd.read_csv(\"/drive/My Drive/dataset_experiment_three.csv\")\n",
        "\n",
        "\n",
        "encoded_sequences = df['content'].apply(encode)\n",
        "padded_sequences = np.array(encoded_sequences.tolist())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(df['tag'])\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(padded_sequences, labels, test_size=0.1, stratify=labels, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.11, stratify=y_temp, random_state=42)\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.build(input_shape=(None, max_length))\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(f\"Test Accuracy: {accuracy:.5f}\")\n",
        "print(f\"Test Loss: {loss:.5f}\")\n",
        "\n",
        "model.save(\"/drive/My Drive/cnn_experiment_three.keras\")"
      ],
      "metadata": {
        "id": "wGtQdOtPGB2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#EXPERIMENT FOUR\n",
        "drive.mount('/drive')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
        "\n",
        "max_length = 512\n",
        "def encode(text):\n",
        "    return tokenizer.encode(text, max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "df = pd.read_json(\"hf://datasets/mihalca/FakeRO_updated/combined_balanced.json\")\n",
        "\n",
        "\n",
        "encoded_sequences = df['content'].apply(encode)\n",
        "padded_sequences = np.array(encoded_sequences.tolist())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(df['tag'])\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.build(input_shape=(None, max_length))\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))\n",
        "print(f\"Test Accuracy: {accuracy:.5f}\")\n",
        "print(f\"Test Loss: {loss:.5f}\")\n",
        "\n",
        "model.save(\"/drive/My Drive/cnn_experiment_four.keras\")"
      ],
      "metadata": {
        "id": "aGZsXsM8GCAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}