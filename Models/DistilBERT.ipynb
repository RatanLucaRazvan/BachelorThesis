{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DAGDbQeAlFj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "\n",
        "#EXPERIMENT ONE\n",
        "drive.mount('/drive')\n",
        "df = pd.read_csv(\"/drive/My Drive/dataset_experiment_one.csv\")\n",
        "\n",
        "label_map = {\"real_news\": 1, \"fake_news\": 0}\n",
        "df[\"label\"] = df[\"tag\"].map(label_map)\n",
        "\n",
        "\n",
        "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
        "    df[\"content\"], df[\"label\"], test_size=0.1, stratify=df[\"label\"], random_state=42\n",
        ")\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_val_texts, train_val_labels, test_size=0.1, stratify=train_val_labels, random_state=42\n",
        ")\n",
        "\n",
        "model_name = \"racai/distilbert-base-romanian-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(texts):\n",
        "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "val_encodings = tokenize(val_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = list(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n",
        "test_dataset = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(model.config.hidden_size, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 2)\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model on: {device}\")\n",
        "\n",
        "stored_preds = []\n",
        "stored_labels = []\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = logits[0] if isinstance(logits, tuple) else logits\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    global stored_preds, stored_labels\n",
        "    stored_preds = preds\n",
        "    stored_labels = labels\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
        "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(preds, labels, class_names=[\"fake_news\", \"real_news\"]):\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/drive/My Drive/results_distilbert_experiment_one\",\n",
        "    logging_dir=\"/drive/My Drive/logs_distilbert_experiment_one\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(metrics)\n",
        "plot_confusion_matrix(stored_preds, stored_labels)\n",
        "\n",
        "model.save_pretrained(\"/drive/My Drive/distilbert_experiment_one\")\n",
        "tokenizer.save_pretrained(\"/drive/My Drive/distilbert_experiment_one\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "\n",
        "#EXPERIMENT TWO\n",
        "\n",
        "drive.mount('/drive')\n",
        "train_df = pd.read_csv(\"/drive/My Drive/dataset_experiment_two_training.csv\")\n",
        "test_df  = pd.read_csv(\"/drive/My Drive/dataset_experiment_two_testing.csv\")\n",
        "\n",
        "label_map = {\"real_news\": 1, \"fake_news\": 0}\n",
        "train_df[\"label\"] = train_df[\"tag\"].map(label_map)\n",
        "test_df[\"label\"] = test_df[\"tag\"].map(label_map)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_df[\"content\"], train_df[\"label\"], test_size=0.1, stratify=train_df[\"label\"], random_state=42\n",
        ")\n",
        "\n",
        "test_texts = test_df[\"content\"]\n",
        "test_labels = test_df[\"label\"]\n",
        "\n",
        "model_name = \"racai/distilbert-base-romanian-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(texts):\n",
        "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "val_encodings = tokenize(val_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = list(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset   = NewsDataset(val_encodings, val_labels)\n",
        "test_dataset  = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(model.config.hidden_size, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 2)\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model on: {device}\")\n",
        "\n",
        "stored_preds = []\n",
        "stored_labels = []\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = logits[0] if isinstance(logits, tuple) else logits\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    global stored_preds, stored_labels\n",
        "    stored_preds = preds\n",
        "    stored_labels = labels\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
        "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(preds, labels, class_names=[\"fake_news\", \"real_news\"]):\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/drive/My Drive/results_distilbert_experiment_two\",\n",
        "    logging_dir=\"/drive/My Drive/logs_distilbert_experiment_two\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(metrics)\n",
        "plot_confusion_matrix(stored_preds, stored_labels)\n",
        "\n",
        "model.save_pretrained(\"/drive/My Drive/distilbert_experiment_two\")\n",
        "tokenizer.save_pretrained(\"/drive/My Drive/distilbert_experiment_two\")\n"
      ],
      "metadata": {
        "id": "qs6xYKP6DSfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "\n",
        "#EXPERIMENT THREE\n",
        "drive.mount('/drive')\n",
        "df = pd.read_csv(\"/drive/My Drive/preprocessed_dataset_original.csv\")\n",
        "\n",
        "label_map = {\"real_news\": 1, \"fake_news\": 0}\n",
        "df[\"label\"] = df[\"tag\"].map(label_map)\n",
        "\n",
        "\n",
        "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
        "    df[\"content\"], df[\"label\"], test_size=0.1, stratify=df[\"label\"], random_state=42\n",
        ")\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_val_texts, train_val_labels, test_size=0.1, stratify=train_val_labels, random_state=42\n",
        ")\n",
        "\n",
        "model_name = \"racai/distilbert-base-romanian-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(texts):\n",
        "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "val_encodings = tokenize(val_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = list(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n",
        "test_dataset = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(model.config.hidden_size, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 2)\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model on: {device}\")\n",
        "\n",
        "stored_preds = []\n",
        "stored_labels = []\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = logits[0] if isinstance(logits, tuple) else logits\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    global stored_preds, stored_labels\n",
        "    stored_preds = preds\n",
        "    stored_labels = labels\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
        "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(preds, labels, class_names=[\"fake_news\", \"real_news\"]):\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/drive/My Drive/results_distilbert_experiment_three\",\n",
        "    logging_dir=\"/drive/My Drive/logs_distilbert_experiment_three\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(metrics)\n",
        "plot_confusion_matrix(stored_preds, stored_labels)\n",
        "\n",
        "model.save_pretrained(\"/drive/My Drive/distilbert_experiment_three\")\n",
        "tokenizer.save_pretrained(\"/drive/My Drive/distilbert_experiment_three\")\n"
      ],
      "metadata": {
        "id": "YIsFKgn3DS9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "\n",
        "#EXPERIMENT FOUR\n",
        "drive.mount('/drive')\n",
        "df = pd.read_json(\"hf://datasets/mihalca/FakeRO_updated/combined_balanced.json\")\n",
        "\n",
        "label_map = {\"real_news\": 1, \"fake_news\": 0, \"propaganda\":2, \"satire\":3, \"misinformation\":4}\n",
        "df[\"label\"] = df[\"tag\"].map(label_map)\n",
        "\n",
        "\n",
        "train_texts, test_val_texts, train_labels, test_val_labels = train_test_split(\n",
        "    df[\"content\"], df[\"label\"], test_size=0.2, stratify=df[\"label\"], random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    test_val_texts, test_val_labels, test_size=0.5, stratify=test_val_labels, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "model_name = \"racai/distilbert-base-romanian-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(texts):\n",
        "    return tokenizer(list(texts), truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "val_encodings = tokenize(val_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = list(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n",
        "test_dataset = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(model.config.hidden_size, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 5)\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model on: {device}\")\n",
        "\n",
        "stored_preds = []\n",
        "stored_labels = []\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = logits[0] if isinstance(logits, tuple) else logits\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    global stored_preds, stored_labels\n",
        "    stored_preds = preds\n",
        "    stored_labels = labels\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, average=\"macro\"),\n",
        "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
        "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(preds, labels, class_names=[\"fake_news\", \"misinformation\", \"propaganda\", \"real_news\",\"satire\"]):\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/drive/My Drive/results_distilbert_experiment_four\",\n",
        "    logging_dir=\"/drive/My Drive/logs_distilbert_experiment_four\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.0,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(metrics)\n",
        "plot_confusion_matrix(stored_preds, stored_labels)\n",
        "\n",
        "model.save_pretrained(\"/drive/My Drive/distilbert_experiment_four\")\n",
        "tokenizer.save_pretrained(\"/drive/My Drive/distilbert_experiment_four\")\n"
      ],
      "metadata": {
        "id": "IscakPyPDTHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}